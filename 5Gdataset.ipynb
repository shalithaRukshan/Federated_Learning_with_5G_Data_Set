{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928dee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-168249438b94>:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_full = df_full.fillna(df_full.median())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "testset_frac = 0.25\n",
    "data_samples = 100000 \n",
    "\n",
    "df_full = pd.read_csv('data/Encoded.csv')\n",
    "df_full = df_full.sample(n=data_samples)\n",
    "df_full = df_full.iloc[:, 1:]\n",
    "df_full = df_full.drop(columns=['Attack Tool', 'Label', 'sVid', 'dVid', '54'])\n",
    "df_full = df_full.fillna(df_full.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aaa4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "numEpoch = 20\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "print_amount=3\n",
    "number_of_slices = 5\n",
    "isSmote = False\n",
    "runtime = 21\n",
    "\n",
    "file_name = \"federated_\" + str(isSmote) + \"_\" + str(number_of_slices)  + \"_\" + str(runtime) + \".txt\"\n",
    "file = open(file_name, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e1c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Benign', 'HTTPFlood', 'ICMPFlood', 'SYNFlood', 'SYNScan',\n",
      "       'SlowrateDoS', 'TCPConnectScan', 'UDPFlood', 'UDPScan'],\n",
      "      dtype='object')\n",
      "Benign            38705\n",
      "UDPFlood          37931\n",
      "HTTPFlood         11733\n",
      "SlowrateDoS        6126\n",
      "SYNScan            1671\n",
      "TCPConnectScan     1612\n",
      "UDPScan            1333\n",
      "SYNFlood            800\n",
      "ICMPFlood            89\n",
      "Name: Attack Type, dtype: int64\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df_full['Attack Type'])  # Classification\n",
    "outcomes = dummies.columns\n",
    "print(outcomes)\n",
    "num_classes = len(outcomes)\n",
    "Y = dummies.values\n",
    "print(df_full['Attack Type'].value_counts())\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5b1585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38705, 90)\n",
      "(11733, 90)\n",
      "(37931, 90)\n",
      "(1671, 90)\n",
      "(6126, 90)\n",
      "0    38705\n",
      "1    37931\n",
      "2    11733\n",
      "3     6126\n",
      "4     1671\n",
      "Name: Attack Type, dtype: int64\n",
      "(96166, 90)\n",
      "(96166,)\n"
     ]
    }
   ],
   "source": [
    "df_Benign = df_full[df_full['Attack Type']=='Benign']\n",
    "df_HTTPFlood= df_full[df_full['Attack Type']=='HTTPFlood']\n",
    "df_UDPFlood= df_full[df_full['Attack Type']=='UDPFlood']\n",
    "df_SYNFlood= df_full[df_full['Attack Type']=='SYNScan']\n",
    "df_SlowrateDoS= df_full[df_full['Attack Type']=='SlowrateDoS']\n",
    "\n",
    "df_Benign.loc[(df_Benign['Attack Type'] == 'Benign'), 'Attack Type'] = 0\n",
    "df_HTTPFlood.loc[(df_HTTPFlood['Attack Type'] == 'HTTPFlood'), 'Attack Type'] = 2\n",
    "df_UDPFlood.loc[(df_UDPFlood['Attack Type'] == 'UDPFlood'), 'Attack Type'] = 1\n",
    "df_SYNFlood.loc[(df_SYNFlood['Attack Type'] == 'SYNScan'), 'Attack Type'] = 4\n",
    "df_SlowrateDoS.loc[(df_SlowrateDoS['Attack Type'] == 'SlowrateDoS'), 'Attack Type'] = 3\n",
    "\n",
    "print(df_Benign.shape)\n",
    "print(df_HTTPFlood.shape)\n",
    "print(df_UDPFlood.shape)\n",
    "print(df_SYNFlood.shape)\n",
    "print(df_SlowrateDoS.shape)\n",
    "\n",
    "df_filterd = pd.concat([df_Benign,df_HTTPFlood,df_UDPFlood,df_SYNFlood,df_SlowrateDoS])\n",
    "print(df_filterd['Attack Type'].value_counts())\n",
    "print(df_filterd.shape)\n",
    "type_df = df_filterd['Attack Type'].copy()\n",
    "data_df = df_filterd.drop('Attack Type',axis=1)\n",
    "print(type_df.shape)\n",
    "\n",
    "\n",
    "data_df = data_df / data_df.max()\n",
    "df_normalized = pd.concat([data_df,type_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d026eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_train_test(df, propotion=0.1):\n",
    "    \n",
    "    df_train = []\n",
    "    df_test = []\n",
    "    for key,val in df['Attack Type'].value_counts().iteritems():\n",
    "        df_part = df[df['Attack Type'] == key]\n",
    "        df_part = df_part.dropna(axis=1)\n",
    "        df_test.append(df_part[0: int(df_part.shape[0]*propotion)])\n",
    "        df_train.append(df_part[int(df_part.shape[0]*propotion):df_part.shape[0]])\n",
    "        \n",
    "    return df_train,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2faec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_slices(df_train, number_of_slices, isSmote=False, x_name=\"x_train\", y_name=\"y_train\"):\n",
    "    \n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()    \n",
    "    \n",
    "    for i in range(number_of_slices):\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        df_types = []\n",
    "        \n",
    "        for df in df_train:\n",
    "            df_type = df[int(df.shape[0]*i/number_of_slices):int(df.shape[0]*(i+1)/number_of_slices)]\n",
    "            df_types.append(df_type)\n",
    "        \n",
    "        slice_df = pd.concat(df_types)\n",
    "        y_info = slice_df.pop('Attack Type').values\n",
    "        x_info = slice_df.values\n",
    "        y_info = y_info.astype('int')\n",
    "        \n",
    "        if isSmote:\n",
    "            sm = SMOTE(random_state=42)\n",
    "            x_info, y_info = sm.fit_resample(x_info, y_info)\n",
    "        \n",
    "        print('========================================================================================')\n",
    "        print('\\tX part size for slice ' + str(i) + ' is ' + str(x_info.shape))\n",
    "        print('\\tY part size for slice ' + str(i) + ' is ' + str(y_info.shape))\n",
    "        print('Value types of each class in slice : ' + str(i))\n",
    "        print(np.unique(y_info,return_counts=True))\n",
    "        \n",
    "        x_info = torch.tensor(x_info).float()\n",
    "        y_info = torch.tensor(y_info).type(torch.LongTensor)\n",
    "            \n",
    "        x_data_dict.update({xname : x_info})\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da8d839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "========================================================================================\n",
      "\tX part size for slice 0 is (17308, 86)\n",
      "\tY part size for slice 0 is (17308,)\n",
      "Value types of each class in slice : 0\n",
      "(array([0, 1, 2, 3, 4]), array([6967, 6827, 2112, 1102,  300], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 1 is (17311, 86)\n",
      "\tY part size for slice 1 is (17311,)\n",
      "Value types of each class in slice : 1\n",
      "(array([0, 1, 2, 3, 4]), array([6967, 6828, 2112, 1103,  301], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 2 is (17310, 86)\n",
      "\tY part size for slice 2 is (17310,)\n",
      "Value types of each class in slice : 2\n",
      "(array([0, 1, 2, 3, 4]), array([6967, 6827, 2112, 1103,  301], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 3 is (17311, 86)\n",
      "\tY part size for slice 3 is (17311,)\n",
      "Value types of each class in slice : 3\n",
      "(array([0, 1, 2, 3, 4]), array([6967, 6828, 2112, 1103,  301], dtype=int64))\n",
      "========================================================================================\n",
      "\tX part size for slice 4 is (17311, 86)\n",
      "\tY part size for slice 4 is (17311,)\n",
      "Value types of each class in slice : 4\n",
      "(array([0, 1, 2, 3, 4]), array([6967, 6828, 2112, 1103,  301], dtype=int64))\n",
      "Test set size is : x => (9615, 86) y => (9615,)\n",
      "86 5\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = divide_train_test(df_normalized,propotion=0.1)\n",
    "print(len(df_train))\n",
    "# print('Value counts in train set : ')\n",
    "# df_train['Attack Type'].value_counts()\n",
    "# print('Value counts in test set : ')\n",
    "# print(df_test['Attack Type'].value_counts())\n",
    "\n",
    "x_train_dict, y_train_dict = get_data_for_slices(df_train, number_of_slices, isSmote)\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "y_test = df_test.pop('Attack Type').values\n",
    "x_test = df_test.values\n",
    "\n",
    "print('Test set size is : x => ' + str(x_test.shape) + ' y => ' + str(y_test.shape))\n",
    "x_test = torch.tensor(x_test).float()\n",
    "y_test = torch.tensor(y_test.astype('int')).type(torch.LongTensor)\n",
    "\n",
    "inputs = x_test.shape[1]\n",
    "outputs = 5\n",
    "print(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fdb49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,outputs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37778c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7bef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7a19c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89bdf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "        output = model(inputs)  # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output)  # Save Prediction\n",
    "\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels)  # Save Truth\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precisionv = precision_score(y_true,y_pred,average='macro')\n",
    "    recallv = recall_score(y_true,y_pred,average='macro')\n",
    "    print('precision value: '+str(precisionv))\n",
    "    print('recall value: '+ str(recallv))\n",
    "#     df_cm = pd.DataFrame(cf_matrix, index=[i for i in Counter(y_test)],\n",
    "#                          columns=[i for i in Counter(y_test)])\n",
    "#     plt.figure(1)\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.figure(figsize=(12, 7))\n",
    "\n",
    "#     sn.heatmap(df_cm, annot=True).set(xlabel='Predicted label', ylabel='True label')\n",
    "#     plt.savefig('D:\\\\learning\\\\PyTorch\\\\experiment\\\\cf\\\\cf_fl_'+str(self.number_of_slices)+'.png')\n",
    "    print('confusion matrix for normal scenario for slices : ' + str(number_of_slices))\n",
    "    print(cf_matrix)\n",
    "    file.write('\\ncf matrix for slice :' + str(number_of_slices))\n",
    "    file.write('\\n'+str(cf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f880e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_optimizer_criterion_dict(number_of_slices):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_slices):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn(inputs, outputs)\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "323fad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_weights(model_dict, number_of_slices):\n",
    "   \n",
    "    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n",
    "    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n",
    "    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n",
    "    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "    \n",
    "        for i in range(number_of_slices):\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/number_of_slices\n",
    "        fc1_mean_bias = fc1_mean_bias/ number_of_slices\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/number_of_slices\n",
    "        fc2_mean_bias = fc2_mean_bias/ number_of_slices\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/number_of_slices\n",
    "        fc3_mean_bias = fc3_mean_bias/ number_of_slices\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "061de023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights(model_dict, number_of_slices=number_of_slices)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a60def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_local_and_merged_model_performance(number_of_slices):\n",
    "    accuracy_table=pd.DataFrame(data=np.zeros((number_of_slices,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n",
    "    for i in range (number_of_slices):\n",
    "    \n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n",
    "        main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n",
    "    \n",
    "        accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n",
    "        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n",
    "        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n",
    "\n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886cf94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_slices):\n",
    "            print('Updating model :' + name_of_models[i] )\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f476a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train_end_node_process(number_of_slices):\n",
    "    for i in range (number_of_slices): \n",
    "\n",
    "        print('Federated learning for slice '+ str(i+1))\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         valid_ds = TensorDataset(x_valid_dict[name_of_x_valid_sets[i]], y_valid_dict[name_of_y_valid_sets[i]])\n",
    "#         valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "        \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        print(\"Subset\" ,i)\n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "#             valid_loss, valid_accuracy = validation(model, valid_dl, criterion)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "    \n",
    "            print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0894528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_train_end_node_process_without_print(number_of_slices):\n",
    "    for i in range (number_of_slices): \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        for epoch in range(numEpoch):        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e5c8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train_end_node_process_print_some(number_of_slices, print_amount):\n",
    "    for i in range (number_of_slices): \n",
    "        \n",
    "        print('Federated learning for slice '+ str(i+1))\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], \n",
    "                                 y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        if i<print_amount:\n",
    "            print(\"Subset\" ,i)\n",
    "            \n",
    "        for epoch in range(numEpoch):\n",
    "        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "            \n",
    "            if i<print_amount:        \n",
    "                print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "543b83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_model = Net2nn()\n",
    "# initial_optimizer = torch.optim.SGD(initial_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# initial_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "centralized_model = Net2nn(inputs, outputs)\n",
    "centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "centralized_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9177cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Centralized Model ------\n",
      "Training with slice 1 data\n",
      "epoch:   1 | train accuracy:  0.8293 | test accuracy:  0.9716\n",
      "epoch:   2 | train accuracy:  0.9720 | test accuracy:  0.9790\n",
      "epoch:   3 | train accuracy:  0.9791 | test accuracy:  0.9806\n",
      "epoch:   4 | train accuracy:  0.9812 | test accuracy:  0.9811\n",
      "epoch:   5 | train accuracy:  0.9811 | test accuracy:  0.9797\n",
      "epoch:   6 | train accuracy:  0.9839 | test accuracy:  0.9882\n",
      "epoch:   7 | train accuracy:  0.9841 | test accuracy:  0.9886\n",
      "epoch:   8 | train accuracy:  0.9852 | test accuracy:  0.9793\n",
      "epoch:   9 | train accuracy:  0.9860 | test accuracy:  0.9878\n",
      "epoch:  10 | train accuracy:  0.9872 | test accuracy:  0.9877\n",
      "epoch:  11 | train accuracy:  0.9863 | test accuracy:  0.9809\n",
      "epoch:  12 | train accuracy:  0.9872 | test accuracy:  0.9903\n",
      "epoch:  13 | train accuracy:  0.9871 | test accuracy:  0.9901\n",
      "epoch:  14 | train accuracy:  0.9854 | test accuracy:  0.9878\n",
      "epoch:  15 | train accuracy:  0.9880 | test accuracy:  0.9794\n",
      "epoch:  16 | train accuracy:  0.9880 | test accuracy:  0.9891\n",
      "epoch:  17 | train accuracy:  0.9892 | test accuracy:  0.9884\n",
      "epoch:  18 | train accuracy:  0.9884 | test accuracy:  0.9897\n",
      "epoch:  19 | train accuracy:  0.9888 | test accuracy:  0.9879\n",
      "epoch:  20 | train accuracy:  0.9882 | test accuracy:  0.9867\n",
      " | train accuracy:  0.9882 | test accuracy:  0.9867\n",
      "precision value: 0.9817979404293915\n",
      "recall value: 0.9844004721766373\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3791   64    5   10    0]\n",
      " [   5 3788    0    0    0]\n",
      " [   0    0 1153   20    0]\n",
      " [   0    0   24  588    0]\n",
      " [   0    0    0    0  167]]\n",
      "Training with slice 2 data\n",
      "epoch:   1 | train accuracy:  0.8259 | test accuracy:  0.9673\n",
      "epoch:   2 | train accuracy:  0.9725 | test accuracy:  0.9839\n",
      "epoch:   3 | train accuracy:  0.9778 | test accuracy:  0.9801\n",
      "epoch:   4 | train accuracy:  0.9771 | test accuracy:  0.9818\n",
      "epoch:   5 | train accuracy:  0.9796 | test accuracy:  0.9862\n",
      "epoch:   6 | train accuracy:  0.9809 | test accuracy:  0.9785\n",
      "epoch:   7 | train accuracy:  0.9820 | test accuracy:  0.9860\n",
      "epoch:   8 | train accuracy:  0.9833 | test accuracy:  0.9860\n",
      "epoch:   9 | train accuracy:  0.9826 | test accuracy:  0.9887\n",
      "epoch:  10 | train accuracy:  0.9846 | test accuracy:  0.9897\n",
      "epoch:  11 | train accuracy:  0.9861 | test accuracy:  0.9903\n",
      "epoch:  12 | train accuracy:  0.9837 | test accuracy:  0.9865\n",
      "epoch:  13 | train accuracy:  0.9833 | test accuracy:  0.9851\n",
      "epoch:  14 | train accuracy:  0.9839 | test accuracy:  0.9861\n",
      "epoch:  15 | train accuracy:  0.9864 | test accuracy:  0.9896\n",
      "epoch:  16 | train accuracy:  0.9865 | test accuracy:  0.9885\n",
      "epoch:  17 | train accuracy:  0.9857 | test accuracy:  0.9871\n",
      "epoch:  18 | train accuracy:  0.9859 | test accuracy:  0.9912\n",
      "epoch:  19 | train accuracy:  0.9857 | test accuracy:  0.9854\n",
      "epoch:  20 | train accuracy:  0.9876 | test accuracy:  0.9897\n",
      " | train accuracy:  0.9876 | test accuracy:  0.9897\n",
      "precision value: 0.9762433728630804\n",
      "recall value: 0.986275054995172\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3831   19    5   11    4]\n",
      " [  10 3783    0    0    0]\n",
      " [   0    0 1140   33    0]\n",
      " [   0    0   17  595    0]\n",
      " [   0    0    0    0  167]]\n",
      "Training with slice 3 data\n",
      "epoch:   1 | train accuracy:  0.8528 | test accuracy:  0.9741\n",
      "epoch:   2 | train accuracy:  0.9753 | test accuracy:  0.9787\n",
      "epoch:   3 | train accuracy:  0.9780 | test accuracy:  0.9846\n",
      "epoch:   4 | train accuracy:  0.9816 | test accuracy:  0.9822\n",
      "epoch:   5 | train accuracy:  0.9806 | test accuracy:  0.9795\n",
      "epoch:   6 | train accuracy:  0.9826 | test accuracy:  0.9844\n",
      "epoch:   7 | train accuracy:  0.9842 | test accuracy:  0.9864\n",
      "epoch:   8 | train accuracy:  0.9843 | test accuracy:  0.9864\n",
      "epoch:   9 | train accuracy:  0.9856 | test accuracy:  0.9854\n",
      "epoch:  10 | train accuracy:  0.9864 | test accuracy:  0.9840\n",
      "epoch:  11 | train accuracy:  0.9846 | test accuracy:  0.9784\n",
      "epoch:  12 | train accuracy:  0.9852 | test accuracy:  0.9893\n",
      "epoch:  13 | train accuracy:  0.9856 | test accuracy:  0.9866\n",
      "epoch:  14 | train accuracy:  0.9869 | test accuracy:  0.9896\n",
      "epoch:  15 | train accuracy:  0.9872 | test accuracy:  0.9882\n",
      "epoch:  16 | train accuracy:  0.9868 | test accuracy:  0.9874\n",
      "epoch:  17 | train accuracy:  0.9871 | test accuracy:  0.9919\n",
      "epoch:  18 | train accuracy:  0.9879 | test accuracy:  0.9898\n",
      "epoch:  19 | train accuracy:  0.9880 | test accuracy:  0.9902\n",
      "epoch:  20 | train accuracy:  0.9901 | test accuracy:  0.9925\n",
      " | train accuracy:  0.9901 | test accuracy:  0.9925\n",
      "precision value: 0.9877481063071267\n",
      "recall value: 0.9874831644476636\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3855    6    7    2    0]\n",
      " [  16 3777    0    0    0]\n",
      " [   0    0 1157   16    0]\n",
      " [   2    0   23  587    0]\n",
      " [   0    0    0    0  167]]\n",
      "Training with slice 4 data\n",
      "epoch:   1 | train accuracy:  0.8420 | test accuracy:  0.9657\n",
      "epoch:   2 | train accuracy:  0.9750 | test accuracy:  0.9814\n",
      "epoch:   3 | train accuracy:  0.9791 | test accuracy:  0.9812\n",
      "epoch:   4 | train accuracy:  0.9811 | test accuracy:  0.9841\n",
      "epoch:   5 | train accuracy:  0.9806 | test accuracy:  0.9872\n",
      "epoch:   6 | train accuracy:  0.9815 | test accuracy:  0.9877\n",
      "epoch:   7 | train accuracy:  0.9825 | test accuracy:  0.9848\n",
      "epoch:   8 | train accuracy:  0.9856 | test accuracy:  0.9796\n",
      "epoch:   9 | train accuracy:  0.9838 | test accuracy:  0.9657\n",
      "epoch:  10 | train accuracy:  0.9856 | test accuracy:  0.9833\n",
      "epoch:  11 | train accuracy:  0.9847 | test accuracy:  0.9879\n",
      "epoch:  12 | train accuracy:  0.9865 | test accuracy:  0.9880\n",
      "epoch:  13 | train accuracy:  0.9871 | test accuracy:  0.9804\n",
      "epoch:  14 | train accuracy:  0.9873 | test accuracy:  0.9847\n",
      "epoch:  15 | train accuracy:  0.9861 | test accuracy:  0.9868\n",
      "epoch:  16 | train accuracy:  0.9880 | test accuracy:  0.9842\n",
      "epoch:  17 | train accuracy:  0.9872 | test accuracy:  0.9898\n",
      "epoch:  18 | train accuracy:  0.9889 | test accuracy:  0.9864\n",
      "epoch:  19 | train accuracy:  0.9891 | test accuracy:  0.9876\n",
      "epoch:  20 | train accuracy:  0.9868 | test accuracy:  0.9860\n",
      " | train accuracy:  0.9868 | test accuracy:  0.9860\n",
      "precision value: 0.9722617642808803\n",
      "recall value: 0.983225191144103\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3804   49    4   10    3]\n",
      " [   5 3788    0    0    0]\n",
      " [   0    0 1123   50    0]\n",
      " [   0    0   14  598    0]\n",
      " [   0    0    0    0  167]]\n",
      "Training with slice 5 data\n",
      "epoch:   1 | train accuracy:  0.8470 | test accuracy:  0.9728\n",
      "epoch:   2 | train accuracy:  0.9760 | test accuracy:  0.9847\n",
      "epoch:   3 | train accuracy:  0.9822 | test accuracy:  0.9811\n",
      "epoch:   4 | train accuracy:  0.9830 | test accuracy:  0.9826\n",
      "epoch:   5 | train accuracy:  0.9835 | test accuracy:  0.9850\n",
      "epoch:   6 | train accuracy:  0.9845 | test accuracy:  0.9872\n",
      "epoch:   7 | train accuracy:  0.9844 | test accuracy:  0.9756\n",
      "epoch:   8 | train accuracy:  0.9868 | test accuracy:  0.9801\n",
      "epoch:   9 | train accuracy:  0.9863 | test accuracy:  0.9871\n",
      "epoch:  10 | train accuracy:  0.9866 | test accuracy:  0.9856\n",
      "epoch:  11 | train accuracy:  0.9884 | test accuracy:  0.9892\n",
      "epoch:  12 | train accuracy:  0.9860 | test accuracy:  0.9876\n",
      "epoch:  13 | train accuracy:  0.9871 | test accuracy:  0.9882\n",
      "epoch:  14 | train accuracy:  0.9878 | test accuracy:  0.9904\n",
      "epoch:  15 | train accuracy:  0.9884 | test accuracy:  0.9869\n",
      "epoch:  16 | train accuracy:  0.9871 | test accuracy:  0.9873\n",
      "epoch:  17 | train accuracy:  0.9890 | test accuracy:  0.9915\n",
      "epoch:  18 | train accuracy:  0.9888 | test accuracy:  0.9850\n",
      "epoch:  19 | train accuracy:  0.9896 | test accuracy:  0.9876\n",
      "epoch:  20 | train accuracy:  0.9901 | test accuracy:  0.9884\n",
      " | train accuracy:  0.9901 | test accuracy:  0.9884\n",
      "precision value: 0.979035330827999\n",
      "recall value: 0.9839766928797706\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3844    7    6    9    4]\n",
      " [  41 3752    0    0    0]\n",
      " [   0    0 1159   14    0]\n",
      " [   0    0   31  581    0]\n",
      " [   0    0    0    0  167]]\n",
      "------ Training finished ------\n",
      "Mean train accuracy: 0.9773890578543463\n",
      "Mean test accuracy: 0.9847758710348414\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Centralized Model ------\")\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "\n",
    "for i in range(number_of_slices):\n",
    "    centralized_model = Net2nn(inputs,outputs)\n",
    "    centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    centralized_criterion = nn.CrossEntropyLoss()\n",
    "#     centralized_model = copy.deepcopy(initial_model)\n",
    "#     centralized_optimizer = copy.deepcopy(initial_optimizer)\n",
    "#     centralized_criterion = copy.deepcopy(initial_criterion)\n",
    "    print('Training with slice ' + str(i+1) + ' data' )\n",
    "    x_name = 'x_train' + str(i)\n",
    "    y_name = 'y_train' + str(i)\n",
    "    train_ds = TensorDataset(x_train_dict[x_name], y_train_dict[y_name])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(numEpoch):\n",
    "        central_train_loss, central_train_accuracy = train(centralized_model, train_dl, centralized_criterion, centralized_optimizer)\n",
    "        central_test_loss, central_test_accuracy = validation(centralized_model, test_dl, centralized_criterion)\n",
    "        \n",
    "        train_acc.append(central_train_accuracy)\n",
    "        train_loss.append(central_train_loss)\n",
    "        test_acc.append(central_test_accuracy)\n",
    "        test_loss.append(central_test_loss)\n",
    "        \n",
    "        print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "    print(\" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "    confusion_mat(centralized_model, test_dl)\n",
    "    \n",
    "print(\"------ Training finished ------\")\n",
    "print('Mean train accuracy: ' + str(sum(train_acc)/len(train_acc)))\n",
    "print('Mean test accuracy: ' + str(sum(test_acc)/len(test_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f3b92e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.write('\\nCentralized Mean train accuracy: ' + str(sum(train_acc)/len(train_acc)))\n",
    "file.write('\\nCentralized Mean test accuracy: ' + str(sum(test_acc)/len(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9851821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17311, 86]) torch.Size([17311])\n",
      "torch.Size([9615, 86]) torch.Size([9615])\n"
     ]
    }
   ],
   "source": [
    "print(x_train_dict[\"x_train1\"].shape, y_train_dict[\"y_train1\"].shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5961c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = Net2nn(inputs,outputs)\n",
    "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49f5495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59415549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_train0', 'x_train1', 'x_train2', 'x_train3', 'x_train4']\n",
      "['y_train0', 'y_train1', 'y_train2', 'y_train3', 'y_train4']\n",
      "\n",
      " ------------\n",
      "['model0', 'model1', 'model2', 'model3', 'model4']\n",
      "['optimizer0', 'optimizer1', 'optimizer2', 'optimizer3', 'optimizer4']\n",
      "['criterion0', 'criterion1', 'criterion2', 'criterion3', 'criterion4']\n"
     ]
    }
   ],
   "source": [
    "name_of_x_train_sets=list(x_train_dict.keys())\n",
    "name_of_y_train_sets=list(y_train_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())\n",
    "\n",
    "print(name_of_x_train_sets)\n",
    "print(name_of_y_train_sets)\n",
    "print(\"\\n ------------\")\n",
    "print(name_of_models)\n",
    "print(name_of_optimizers)\n",
    "print(name_of_criterions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17d7afff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0318, -0.0075, -0.0096, -0.0560,  0.0213]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[ 0.0103,  0.0375, -0.0418, -0.0335, -0.0655]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6464bd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n"
     ]
    }
   ],
   "source": [
    "model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec2e1051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0318, -0.0075, -0.0096, -0.0560,  0.0213]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[-0.0318, -0.0075, -0.0096, -0.0560,  0.0213]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7689e7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated learning for slice 1\n",
      "Subset 0\n",
      "epoch:   1 | train accuracy: 0.82037 | test accuracy: 0.97098\n",
      "epoch:   2 | train accuracy: 0.97302 | test accuracy: 0.97608\n",
      "epoch:   3 | train accuracy: 0.97706 | test accuracy: 0.98294\n",
      "epoch:   4 | train accuracy: 0.98128 | test accuracy: 0.97733\n",
      "epoch:   5 | train accuracy: 0.98313 | test accuracy: 0.98461\n",
      "epoch:   6 | train accuracy: 0.98134 | test accuracy: 0.98918\n",
      "epoch:   7 | train accuracy: 0.98492 | test accuracy: 0.98305\n",
      "epoch:   8 | train accuracy: 0.98480 | test accuracy: 0.98721\n",
      "epoch:   9 | train accuracy: 0.98683 | test accuracy: 0.99012\n",
      "epoch:  10 | train accuracy: 0.98804 | test accuracy: 0.98513\n",
      "epoch:  11 | train accuracy: 0.98700 | test accuracy: 0.98690\n",
      "epoch:  12 | train accuracy: 0.98717 | test accuracy: 0.99064\n",
      "epoch:  13 | train accuracy: 0.98792 | test accuracy: 0.98544\n",
      "epoch:  14 | train accuracy: 0.98735 | test accuracy: 0.98762\n",
      "epoch:  15 | train accuracy: 0.98810 | test accuracy: 0.99106\n",
      "epoch:  16 | train accuracy: 0.98856 | test accuracy: 0.98617\n",
      "epoch:  17 | train accuracy: 0.98769 | test accuracy: 0.99106\n",
      "epoch:  18 | train accuracy: 0.98914 | test accuracy: 0.98846\n",
      "epoch:  19 | train accuracy: 0.98891 | test accuracy: 0.98981\n",
      "epoch:  20 | train accuracy: 0.98977 | test accuracy: 0.98804\n",
      "Federated learning for slice 2\n",
      "Subset 1\n",
      "epoch:   1 | train accuracy: 0.82537 | test accuracy: 0.97556\n",
      "epoch:   2 | train accuracy: 0.97285 | test accuracy: 0.97993\n",
      "epoch:   3 | train accuracy: 0.97868 | test accuracy: 0.98253\n",
      "epoch:   4 | train accuracy: 0.97915 | test accuracy: 0.97962\n",
      "epoch:   5 | train accuracy: 0.97886 | test accuracy: 0.98398\n",
      "epoch:   6 | train accuracy: 0.98290 | test accuracy: 0.98575\n",
      "epoch:   7 | train accuracy: 0.98244 | test accuracy: 0.98544\n",
      "epoch:   8 | train accuracy: 0.98469 | test accuracy: 0.98731\n",
      "epoch:   9 | train accuracy: 0.98394 | test accuracy: 0.98554\n",
      "epoch:  10 | train accuracy: 0.98336 | test accuracy: 0.99002\n",
      "epoch:  11 | train accuracy: 0.98440 | test accuracy: 0.97743\n",
      "epoch:  12 | train accuracy: 0.98527 | test accuracy: 0.98742\n",
      "epoch:  13 | train accuracy: 0.98602 | test accuracy: 0.99054\n",
      "epoch:  14 | train accuracy: 0.98590 | test accuracy: 0.98866\n",
      "epoch:  15 | train accuracy: 0.98458 | test accuracy: 0.98430\n",
      "epoch:  16 | train accuracy: 0.98648 | test accuracy: 0.99085\n",
      "epoch:  17 | train accuracy: 0.98735 | test accuracy: 0.98960\n",
      "epoch:  18 | train accuracy: 0.98585 | test accuracy: 0.99158\n",
      "epoch:  19 | train accuracy: 0.98619 | test accuracy: 0.99126\n",
      "epoch:  20 | train accuracy: 0.98781 | test accuracy: 0.98929\n",
      "Federated learning for slice 3\n",
      "Subset 2\n",
      "epoch:   1 | train accuracy: 0.81664 | test accuracy: 0.95798\n",
      "epoch:   2 | train accuracy: 0.97418 | test accuracy: 0.97618\n",
      "epoch:   3 | train accuracy: 0.97915 | test accuracy: 0.98253\n",
      "epoch:   4 | train accuracy: 0.97984 | test accuracy: 0.98450\n",
      "epoch:   5 | train accuracy: 0.98146 | test accuracy: 0.97743\n",
      "epoch:   6 | train accuracy: 0.98365 | test accuracy: 0.98950\n",
      "epoch:   7 | train accuracy: 0.98267 | test accuracy: 0.97754\n",
      "epoch:   8 | train accuracy: 0.98429 | test accuracy: 0.98929\n",
      "epoch:   9 | train accuracy: 0.98423 | test accuracy: 0.98606\n",
      "epoch:  10 | train accuracy: 0.98631 | test accuracy: 0.98908\n",
      "epoch:  11 | train accuracy: 0.98602 | test accuracy: 0.98606\n",
      "epoch:  12 | train accuracy: 0.98694 | test accuracy: 0.98908\n",
      "epoch:  13 | train accuracy: 0.98718 | test accuracy: 0.99095\n",
      "epoch:  14 | train accuracy: 0.98608 | test accuracy: 0.98846\n",
      "epoch:  15 | train accuracy: 0.98793 | test accuracy: 0.97129\n",
      "epoch:  16 | train accuracy: 0.98648 | test accuracy: 0.98700\n",
      "epoch:  17 | train accuracy: 0.98873 | test accuracy: 0.98534\n",
      "epoch:  18 | train accuracy: 0.98798 | test accuracy: 0.99241\n",
      "epoch:  19 | train accuracy: 0.98758 | test accuracy: 0.98794\n",
      "epoch:  20 | train accuracy: 0.98902 | test accuracy: 0.99002\n",
      "Federated learning for slice 4\n",
      "Federated learning for slice 5\n"
     ]
    }
   ],
   "source": [
    "# start_train_end_node_process()\n",
    "start_train_end_node_process_print_some(number_of_slices, print_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23bec51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0318, -0.0075, -0.0096, -0.0560,  0.0213], grad_fn=<SliceBackward>)\n",
      "tensor([-0.0458,  0.1004,  0.0328,  0.0410, -0.0149], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "## As you can see, wieghts of local models are updated after training process\n",
    "print(main_model.fc2.weight[0,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06336116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\anaconda-python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision value: 0.003474100270438943\n",
      "recall value: 0.2\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[   0    1    0    0 3869]\n",
      " [   0    0    0    0 3793]\n",
      " [   0    0    0    0 1173]\n",
      " [   0    0    0    0  612]\n",
      " [   0    0    0    0  167]]\n",
      "precision value: 0.9809979335673242\n",
      "recall value: 0.9855273779439999\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3837   14    6    9    4]\n",
      " [   5 3788    0    0    0]\n",
      " [   0    0 1161   12    0]\n",
      " [   0    0   32  580    0]\n",
      " [   0    0    0    0  167]]\n"
     ]
    }
   ],
   "source": [
    "before_acc_table=compare_local_and_merged_model_performance(number_of_slices=number_of_slices)\n",
    "before_test_loss, before_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "file.write('\\nbefore training main model')\n",
    "confusion_mat(main_model, test_dl)\n",
    "\n",
    "main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices) \n",
    "\n",
    "after_acc_table=compare_local_and_merged_model_performance(number_of_slices=number_of_slices)\n",
    "after_test_loss, after_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "file.write('\\nafter training main model')\n",
    "confusion_mat(main_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f992e8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models before FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.0174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.0174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.0174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.0174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample 4</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.0174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.9880             0.0174\n",
       "1  sample 1           0.9893             0.0174\n",
       "2  sample 2           0.9900             0.0174\n",
       "3  sample 3           0.9856             0.0174\n",
       "4  sample 4           0.9907             0.0174"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models before FedAvg first iteration\")\n",
    "file.write('\\nBefore training federated')\n",
    "file.write('\\n'+str(before_acc_table))\n",
    "before_acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f5ff331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated main model vs individual local models after FedAvg first iteration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>local_ind_model</th>\n",
       "      <th>merged_main_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample 0</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>0.9915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample 1</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample 2</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.9915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample 3</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>0.9915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample 4</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.9915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample  local_ind_model  merged_main_model\n",
       "0  sample 0           0.9880             0.9915\n",
       "1  sample 1           0.9893             0.9915\n",
       "2  sample 2           0.9900             0.9915\n",
       "3  sample 3           0.9856             0.9915\n",
       "4  sample 4           0.9907             0.9915"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Federated main model vs individual local models after FedAvg first iteration\")\n",
    "file.write('\\nAfter training federated')\n",
    "file.write('\\n'+str(after_acc_table))\n",
    "after_acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e66d749d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before 1st iteration main model accuracy on all test data:  0.0174\n",
      "After 1st iteration main model accuracy on all test data:  0.9915\n",
      "Centralized model accuracy on all test data:  0.9884\n"
     ]
    }
   ],
   "source": [
    "print(\"Before 1st iteration main model accuracy on all test data: {:7.4f}\".format(before_test_accuracy))\n",
    "print(\"After 1st iteration main model accuracy on all test data: {:7.4f}\".format(after_test_accuracy))\n",
    "print(\"Centralized model accuracy on all test data: {:7.4f}\".format(central_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93b08114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Iteration 2 : main_model accuracy on all test data:  0.9930\n",
      "precision value: 0.981775585202206\n",
      "recall value: 0.9899070163786196\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3847    6    4    9    4]\n",
      " [   8 3785    0    0    0]\n",
      " [   0    0 1152   21    0]\n",
      " [   0    0   15  597    0]\n",
      " [   0    0    0    0  167]]\n",
      "Updating model :model0\n",
      "Updating model :model1\n",
      "Updating model :model2\n",
      "Updating model :model3\n",
      "Updating model :model4\n",
      "Iteration 3 : main_model accuracy on all test data:  0.9947\n",
      "precision value: 0.9867520515447165\n",
      "recall value: 0.9917646645718019\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3852    5    5    6    2]\n",
      " [   3 3790    0    0    0]\n",
      " [   0    0 1156   17    0]\n",
      " [   0    0   13  599    0]\n",
      " [   0    0    0    0  167]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_slices)\n",
    "    start_train_end_node_process_without_print(number_of_slices)\n",
    "    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_slices) \n",
    "    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "    print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy)) \n",
    "    confusion_mat(main_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "234ecd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision value: 0.9867520515447165\n",
      "recall value: 0.9917646645718019\n",
      "confusion matrix for normal scenario for slices : 5\n",
      "[[3852    5    5    6    2]\n",
      " [   3 3790    0    0    0]\n",
      " [   0    0 1156   17    0]\n",
      " [   0    0   13  599    0]\n",
      " [   0    0    0    0  167]]\n"
     ]
    }
   ],
   "source": [
    "confusion_mat(main_model, test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
