{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928dee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0c734b6eda91>:45: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_full = df_full.fillna(df_full.median())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "testset_frac = 0.25\n",
    "data_samples = 80000 \n",
    "\n",
    "df_full = pd.read_csv('data/Encoded.csv')\n",
    "df_full = df_full.sample(n=data_samples)\n",
    "df_full = df_full.iloc[:, 1:]\n",
    "df_full = df_full.drop(columns=['Attack Tool', 'Label', 'sVid', 'dVid', '54'])\n",
    "df_full = df_full.fillna(df_full.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6a3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "numEpoch = 20\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "print_amount=3\n",
    "number_of_slices = 1\n",
    "isSmote = False\n",
    "runtime = 21\n",
    "\n",
    "file_name = \"federated_\" + str(isSmote) + \"_\" + str(number_of_slices)  + \"_\" + str(runtime) + \".txt\"\n",
    "file = open(file_name, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e1c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Benign', 'HTTPFlood', 'ICMPFlood', 'SYNFlood', 'SYNScan',\n",
      "       'SlowrateDoS', 'TCPConnectScan', 'UDPFlood', 'UDPScan'],\n",
      "      dtype='object')\n",
      "Benign            31406\n",
      "UDPFlood          30087\n",
      "HTTPFlood          9299\n",
      "SlowrateDoS        4770\n",
      "SYNScan            1323\n",
      "TCPConnectScan     1290\n",
      "UDPScan            1079\n",
      "SYNFlood            668\n",
      "ICMPFlood            78\n",
      "Name: Attack Type, dtype: int64\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df_full['Attack Type'])  # Classification\n",
    "outcomes = dummies.columns\n",
    "print(outcomes)\n",
    "num_classes = len(outcomes)\n",
    "Y = dummies.values\n",
    "print(df_full['Attack Type'].value_counts())\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5b1585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31406, 90)\n",
      "(9299, 90)\n",
      "(30087, 90)\n",
      "(1323, 90)\n",
      "(4770, 90)\n",
      "0    31406\n",
      "1    30087\n",
      "2     9299\n",
      "3     4770\n",
      "4     1323\n",
      "Name: Attack Type, dtype: int64\n",
      "(76885, 90)\n",
      "(76885,)\n"
     ]
    }
   ],
   "source": [
    "df_Benign = df_full[df_full['Attack Type']=='Benign']\n",
    "df_HTTPFlood= df_full[df_full['Attack Type']=='HTTPFlood']\n",
    "df_UDPFlood= df_full[df_full['Attack Type']=='UDPFlood']\n",
    "df_SYNFlood= df_full[df_full['Attack Type']=='SYNScan']\n",
    "df_SlowrateDoS= df_full[df_full['Attack Type']=='SlowrateDoS']\n",
    "\n",
    "df_Benign.loc[(df_Benign['Attack Type'] == 'Benign'), 'Attack Type'] = 0\n",
    "df_HTTPFlood.loc[(df_HTTPFlood['Attack Type'] == 'HTTPFlood'), 'Attack Type'] = 2\n",
    "df_UDPFlood.loc[(df_UDPFlood['Attack Type'] == 'UDPFlood'), 'Attack Type'] = 1\n",
    "df_SYNFlood.loc[(df_SYNFlood['Attack Type'] == 'SYNScan'), 'Attack Type'] = 4\n",
    "df_SlowrateDoS.loc[(df_SlowrateDoS['Attack Type'] == 'SlowrateDoS'), 'Attack Type'] = 3\n",
    "\n",
    "print(df_Benign.shape)\n",
    "print(df_HTTPFlood.shape)\n",
    "print(df_UDPFlood.shape)\n",
    "print(df_SYNFlood.shape)\n",
    "print(df_SlowrateDoS.shape)\n",
    "\n",
    "df_filterd = pd.concat([df_Benign,df_HTTPFlood,df_UDPFlood,df_SYNFlood,df_SlowrateDoS])\n",
    "print(df_filterd['Attack Type'].value_counts())\n",
    "print(df_filterd.shape)\n",
    "type_df = df_filterd['Attack Type'].copy()\n",
    "data_df = df_filterd.drop('Attack Type',axis=1)\n",
    "print(type_df.shape)\n",
    "\n",
    "\n",
    "data_df = data_df / data_df.max()\n",
    "df_normalized = pd.concat([data_df,type_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d026eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_train_test(df, propotion=0.1):\n",
    "    \n",
    "    df_train = []\n",
    "    df_test = []\n",
    "    for key,val in df['Attack Type'].value_counts().iteritems():\n",
    "        df_part = df[df['Attack Type'] == key]\n",
    "        df_test.append(df_part[0: int(df_part.shape[0]*propotion)])\n",
    "        df_train.append(df_part[int(df_part.shape[0]*propotion):df_part.shape[0]])\n",
    "        \n",
    "    return df_train,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da8d839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69199, 90)\n",
      "(29457, 85)\n",
      "Test set size is : x => (7686, 84) y => (7686,)\n",
      "84\n",
      "84 5\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = divide_train_test(df_normalized,propotion=0.1)\n",
    "\n",
    "\n",
    "df_train = pd.concat(df_train)\n",
    "print(df_train.shape)\n",
    "df_train=df_train.dropna(axis=1)\n",
    "\n",
    "# df_train = df_train.drop(df_train.loc[df_train['Attack Type']==1].index, inplace=True)\n",
    "# df_train = df_train.loc[df_train['Attack Type']!=1]\n",
    "df_train = df_train.loc[df_train['Attack Type']!=1]\n",
    "df_train = df_train.loc[df_train['Attack Type']!=2]\n",
    "df_train = df_train.loc[df_train['Attack Type']!=3]\n",
    "print(df_train.shape)\n",
    "\n",
    "\n",
    "df_test = pd.concat(df_test)\n",
    "df_test=df_test.dropna(axis=1)\n",
    "\n",
    "\n",
    "y_test = df_test.pop('Attack Type').values\n",
    "x_test = df_test.values\n",
    "\n",
    "print('Test set size is : x => ' + str(x_test.shape) + ' y => ' + str(y_test.shape))\n",
    "x_test = torch.tensor(x_test).float()\n",
    "y_test = torch.tensor(y_test.astype('int')).type(torch.LongTensor)\n",
    "\n",
    "y_info = df_train.pop('Attack Type').values\n",
    "# print(np.isnan(y_info))\n",
    "x_info = df_train.values\n",
    "\n",
    "y_info = y_info.astype('int')\n",
    "x_train = torch.tensor(x_info).float()\n",
    "y_train = torch.tensor(y_info).type(torch.LongTensor)        \n",
    "\n",
    "print(x_train.shape[1])\n",
    "inputs = x_test.shape[1]\n",
    "outputs = 5\n",
    "print(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fdb49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,outputs)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f7bef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "#         print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a19c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89bdf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "        output = model(inputs)  # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output)  # Save Prediction\n",
    "\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels)  # Save Truth\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precisionv = precision_score(y_true,y_pred,average='macro')\n",
    "    recallv = recall_score(y_true,y_pred,average='macro')\n",
    "    print('precision value: '+str(precisionv))\n",
    "    print('recall value: '+ str(recallv))\n",
    "#     df_cm = pd.DataFrame(cf_matrix, index=[i for i in Counter(y_test)],\n",
    "#                          columns=[i for i in Counter(y_test)])\n",
    "#     plt.figure(1)\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.figure(figsize=(12, 7))\n",
    "\n",
    "#     sn.heatmap(df_cm, annot=True).set(xlabel='Predicted label', ylabel='True label')\n",
    "#     plt.savefig('D:\\\\learning\\\\PyTorch\\\\experiment\\\\cf\\\\cf_fl_'+str(self.number_of_slices)+'.png')\n",
    "    print('confusion matrix for normal scenario for slices : ' + str(number_of_slices))\n",
    "    print(cf_matrix)\n",
    "    file.write('\\ncf matrix for slice :' + str(number_of_slices))\n",
    "    file.write('\\n'+str(cf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543b83bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n",
      "0.4257090814467864\n"
     ]
    }
   ],
   "source": [
    "# initial_model = Net2nn()\n",
    "# initial_optimizer = torch.optim.SGD(initial_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# initial_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "centralized_model = Net2nn(inputs, outputs)\n",
    "centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "centralized_criterion = nn.CrossEntropyLoss()\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "\n",
    "for epoch in range(numEpoch):\n",
    "    central_train_loss, central_train_accuracy = train(centralized_model, train_dl, centralized_criterion, centralized_optimizer)\n",
    "    central_test_loss, central_test_accuracy = validation(centralized_model, test_dl, centralized_criterion)\n",
    "\n",
    "    train_acc.append(central_train_accuracy)\n",
    "    train_loss.append(central_train_loss)\n",
    "    test_acc.append(central_test_accuracy)\n",
    "    test_loss.append(central_test_loss)\n",
    "    print(central_test_accuracy)\n",
    "#     print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f5e3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision value: 0.2831347630394493\n",
      "recall value: 0.4\n",
      "confusion matrix for normal scenario for slices : 1\n",
      "[[3140    0    0    0    0]\n",
      " [3008    0    0    0    0]\n",
      " [ 929    0    0    0    0]\n",
      " [ 477    0    0    0    0]\n",
      " [   0    0    0    0  132]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\anaconda-python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "confusion_mat(centralized_model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e2dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a78d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
